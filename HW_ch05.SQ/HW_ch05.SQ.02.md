### This homework is written by Nguyen Xuan Thuan (412707007)

#### Special Question: Show that when K=2, b has the same sampling distribution as b1 and b2 in Chapter 2.

#### Answer

Given the vector equation $$Y = X\beta + e,$$ where $Y$ is a $N \times 1$ vector, $\beta$ is a $K \times 1$ vector, $e$ is a $N \times 1$ vector of errors, and $X$ is a $N \times K$ matrix of explanatory variables. The matrix $X$ includes a column of ones for the intercept, and considering $K = 2$, it implies that we have one intercept and one other variable. Thus, the matrix $X$ can be expressed as:

$$
X = \begin{pmatrix}
1 & x_{1,2} \\
1 & x_{2,2} \\
\vdots & \vdots \\
1 & x_{N,2}
\end{pmatrix}
$$

To compute the explicit formula for $(X'X)^{-1}$, we first calculate $X'X$. The transpose of $X$ is:

$$
X' = \begin{pmatrix}
1 & 1 & \cdots & 1 \\
x_{1,2} & x_{2,2} & \cdots & x_{N,2}
\end{pmatrix}
$$

Multiplying $X'$ and $X$ results in:

$$
X'X = \begin{pmatrix}
1 & 1 & \cdots & 1 \\
x_{1,2} & x_{2,2} & \cdots & x_{N,2}
\end{pmatrix}
\begin{pmatrix}
1 & x_{1,2} \\
1 & x_{2,2} \\
\vdots & \vdots \\
1 & x_{N,2}
\end{pmatrix} 
= \begin{pmatrix}
N & \sum_{i=1}^{N} x_{i,2} \\
\sum_{i=1}^{N} x_{i,2} & \sum_{i=1}^{N} x_{i,2}^2
\end{pmatrix}
$$

To find the inverse of a $2 \times 2$ matrix, 

$$
A = \begin{pmatrix} a & b \\
c & d
\end{pmatrix}
$$

the inverse is given by:

$$
A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} d & -b \\
-c & a
\end{pmatrix}
$$

Applying this to our matrix $X'X$, where $a = N$, $b = c = \sum_{i=1}^{N} (x_{i,2})$, and $d = \sum_{i=1}^{N} (x_{i,2})^2$, we have:

$$
(X'X)^{-1} = \frac{1}{N \sum_{i=1}^{N} x_{i,2}^2 - (\sum_{i=1}^{N} x_{i,2})^2} \begin{pmatrix}
\sum_{i=1}^{N} x_{i,2}^2 & -\sum_{i=1}^{N} x_{i,2} \\
-\sum_{i=1}^{N} x_{i,2} & N
\end{pmatrix}
$$

This formula gives us the inverse of $X'X$, which is crucial in the calculation of the estimator for $\beta$ in the ordinary least squares regression model:

$$
b = (X'X)^{-1}(X'Y)
$$

This completes the derivation for the explicit formula of $(X'X)^{-1}$ in the case when $K = 2$.


#### Derive $b=(X'X)^{-1}(X'Y)$ element by element for $K=2$.

To derive the elements of $b = (X'X)^{-1}(X'Y)$ for $K = 2$, we first need the individual elements of $(X'X)^{-1}$ and $X'Y$.

### 1. Calculate $X'X$ and $(X'X)^{-1}$

As previously derived, for $K = 2$ (one intercept and one regressor):

$$
X'X = \begin{pmatrix}
N & \sum x_{i,2} \\
\sum x_{i,2} & \sum x_{i,2}^2
\end{pmatrix}
$$

The inverse, $(X'X)^{-1}$, is given by:

$$
(X'X)^{-1} = \frac{1}{\Delta} \begin{pmatrix}
\sum x_{i,2}^2 & -\sum x_{i,2} \\
-\sum x_{i,2} & N
\end{pmatrix}
$$

where $\Delta = N \sum x_{i,2}^2 - (\sum x_{i,2})^2$.

### 2. Calculate $X'Y$

$$
X'Y = \begin{pmatrix}
1 & 1 & \cdots & 1 \\
x_{1,2} & x_{2,2} & \cdots & x_{N,2}
\end{pmatrix}
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{pmatrix}
= \begin{pmatrix}
\sum y_i \\
\sum x_{i,2} y_i
\end{pmatrix}
$$

### 3. Calculate $b = (X'X)^{-1}(X'Y)$

We now compute the product of $(X'X)^{-1}$ and $X'Y$:

$$
b = \frac{1}{\Delta} \begin{pmatrix}
\sum x_{i,2}^2 & -\sum x_{i,2} \\
-\sum x_{i,2} & N
\end{pmatrix}
\begin{pmatrix}
\sum y_i \\
\sum x_{i,2} y_i
\end{pmatrix}
$$

Let's explicitly calculate the elements of $b$:

**Element $b_1$** (intercept):

$$
b_1 = \frac{1}{\Delta} \left( \left(\sum x_{i,2}^2\right) \left(\sum y_i\right) - \left(\sum x_{i,2}\right) \left(\sum x_{i,2} y_i\right) \right)
$$

**Element $b_2$** (slope):

$$
b_2 = \frac{1}{\Delta} \left( -\left(\sum x_{i,2}\right) \left(\sum y_i\right) + N \left(\sum x_{i,2} y_i\right) \right)
$$

This gives us the two elements of $b$, where $b_1$ is the estimated intercept and $b_2$ is the estimated slope of the regression line. Each element is calculated by the respective formula, reflecting how each factor (intercept and slope) is influenced by the sums of $y_i$, $x_{i,2}$, and their products, scaled by the determinant $\Delta$ of the matrix $(X'X)^{-1}$.

#### Derive covariance matrix of $b$

To derive the covariance matrix of the estimator $b$ in the context of a linear regression model where $K = 2$ (including an intercept and one regressor), we use the formula:

$$
\text{Cov}(b) = (X'X)^{-1}\sigma^2
$$

Here, $\sigma^2$ is the variance of the error terms $e$ in the model $Y = X\beta + e$. The matrix $(X'X)^{-1}$ was previously calculated as:

$$
(X'X)^{-1} = \frac{1}{\Delta} \begin{pmatrix}
\sum x_{i,2}^2 & -\sum x_{i,2} \\
-\sum x_{i,2} & N
\end{pmatrix}
$$

where $\Delta = N \sum x_{i,2}^2 - (\sum x_{i,2})^2$.

### Formula for $\text{Cov}(b)$

Multiplying $(X'X)^{-1}$ by $\sigma^2$, we obtain:

$$
\text{Cov}(b) = \sigma^2 \frac{1}{\Delta} \begin{pmatrix}
\sum x_{i,2}^2 & -\sum x_{i,2} \\
-\sum x_{i,2} & N
\end{pmatrix}
= \frac{\sigma^2}{\Delta} \begin{pmatrix}
\sum x_{i,2}^2 & -\sum x_{i,2} \\
-\sum x_{i,2} & N
\end{pmatrix}
$$

### Elements of the Covariance Matrix

This results in the covariance matrix $\text{Cov}(b)$ having the following elements:

- **Variance of $b_1$ (intercept)**:

$$
\text{Var}(b_1) = \frac{\sigma^2 \sum x_{i,2}^2}{\Delta}
$$

- **Variance of $b_2$ (slope)**:

$$
\text{Var}(b_2) = \frac{\sigma^2 N}{\Delta}
$$

- **Covariance of $b_1$ and $b_2$**:

$$
\text{Cov}(b_1, b_2) = \frac{-\sigma^2 \sum x_{i,2}}{\Delta}
$$

Thus, the covariance matrix of $b$ captures how the variances of the estimates of the intercept and slope are scaled by $\sigma^2$ and adjusted for the design matrix $X$ through $(X'X)^{-1}$. The off-diagonal elements represent the covariance between the intercept and the slope, indicating how changes in one parameter estimate are statistically associated with changes in the other.
